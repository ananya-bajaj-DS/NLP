# -*- coding: utf-8 -*-
"""May27.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1snV-q2Xiaymnf4hjGeXb5deHFotiacWn
"""

# !ls snli_1.0

#Installing the needed Libraries


#Loading the dataset into a train variable
import json
from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-base')

with open('snli_1.0_train.jsonl', 'r') as f:
    snli_train = [json.loads(line) for line in f]

print(snli_train[:5])

#eval
with open('snli_1.0_dev.jsonl', 'r') as f:
    snli_eval = [json.loads(line) for line in f]

print(snli_eval[:5])

#test
with open('snli_1.0_test.jsonl', 'r') as f:
    snli_test = [json.loads(line) for line in f]

print(snli_test[:5])

#Temporarily taking up only 5 examples for training
snli_train=snli_train[:5]

snli_eval=snli_eval[:5]

snli_test=snli_test[:5]

snli_train
snli_eval
snli_test

#Tokenizing completely
from transformers import T5Tokenizer
import torch

tokenizer = T5Tokenizer.from_pretrained('t5-base')

#Special Tokens for concatenating the hypothesis and premise [SEP] will separate the two sentences, [CLS] is in the beginning - forgot what it does
tokenizer.add_special_tokens({'additional_special_tokens': ['[SEP]', '[CLS]']})

#Tokenizer function
def tokenize_function(example):
    label=-1
    premise = example['sentence1']
    hypothesis = example['sentence2']
    input_text = f"[CLS] premise: {premise} [SEP] hypothesis: {hypothesis}"
    encoded_inputs = tokenizer.encode_plus(
        input_text,
        padding='max_length',
        truncation=True,
        max_length=1500,
        return_tensors='pt',
        return_attention_mask=True
    )
    print("encoded inp 1: ", encoded_inputs)

    # Map gold_label to 0, 1, and 2
    gold_label = example['gold_label']
    if gold_label == 'entailment':
        label = 0
    elif gold_label == 'contradiction':
        label = 1
    elif gold_label == 'neutral':
        label = 2

    encoded_inputs['label'] = torch.tensor(label)
    encoded_inputs['text'] = input_text

    print("encoded inp 2:", encoded_inputs)
    return encoded_inputs

list1=[]

# Tokenize the examples individually and append the values to the lists
for example in snli_train:
    tokenized_example = tokenize_function(example)
    list1.append(tokenized_example)

print(list1[1])


list_eval=[]

# Tokenize the examples individually and append the values to the lists
for example in snli_eval:
    tokenized_example_eval = tokenize_function(example)
    list_eval.append(tokenized_example_eval)

print(list_eval[1])


list_test=[]

# Tokenize the examples individually and append the values to the lists
for example in snli_test:
    tokenized_example_test = tokenize_function(example)
    list_test.append(tokenized_example_test)

print(list_test[1])

# Create an empty list to store the dictionaries
list_of_dicts = []

# Iterate over the elements in list1 and convert each element to a dictionary
for example in list1:
    dict_example = {
        'label': example['label'],
        'text': example['text'],
        'input_ids': example['input_ids'],
        'attention_mask': example['attention_mask']
    }
    list_of_dicts.append(dict_example)


# Create an empty list to store the dictionaries
list_of_dicts_eval = []

# Iterate over the elements in list1 and convert each element to a dictionary
for example in list_eval:
    dict_example = {
        'label': example['label'],
        'text': example['text'],
        'input_ids': example['input_ids'],
        'attention_mask': example['attention_mask']
    }
    list_of_dicts_eval.append(dict_example)

list_of_dicts
list_of_dicts_eval

#THIS IS DATASET OF DICTS/LISTS (depends how u call it) - Correct

from datasets import Dataset

# Create an empty dictionary to store the data
dataset_dict = {}

# Iterate over the dictionaries in list_of_dicts and populate the dataset_dict
for i, example in enumerate(list_of_dicts):
    for key, value in example.items():
        if key not in dataset_dict:
            dataset_dict[key] = []
        dataset_dict[key].append(value)


# Convert the dataset_dict to a Hugging Face datasets object
dataset = Dataset.from_dict(dataset_dict)

# Print the dataset of dicts/lists - Correct
print(dataset)
########EVAL
dataset_dict_eval = {}

# Iterate over the dictionaries in list_of_dicts and populate the dataset_dict
for i, example in enumerate(list_of_dicts_eval):
    for key, value in example.items():
        if key not in dataset_dict_eval:
            dataset_dict_eval[key] = []
        dataset_dict_eval[key].append(value)


# Convert the dataset_dict to a Hugging Face datasets object
dataset_eval = Dataset.from_dict(dataset_dict_eval)

# Print the dataset of dicts/lists - Correct
print(dataset_eval)


#At this point we have a dataset of lists/dicts as we wanted, now we want datadict of dataset: i think we dont have to cz in the YELP example the datadict consist of train and val and that creates a datadict: So can wait on that

#old notes
#Needed: A datasetdict of datasets of dicts 
#Have: A datasetdict of lists of dicts


from datasets import DatasetDict

dataset_dict = DatasetDict({
    "train": dataset,
    "eval":dataset_eval

})

#Completely matches with yelp!!
print(type(dataset_dict['train']))

#We need a dataset dict of datasets
print(dataset_dict)

#Training
import torch
torch.cuda.empty_cache()
tokenized_datasets = dataset_dict.remove_columns('text')

print(tokenized_datasets)

tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

tokenized_datasets.set_format("torch")

from torch.utils.data import DataLoader

train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=5)
eval_dataloader = DataLoader(tokenized_datasets["eval"], shuffle=True, batch_size=5)
# eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)

print(train_dataloader)

for batch in train_dataloader:
    for key, value in batch.items():
        print(key, value)

from transformers import T5ForConditionalGeneration, T5Tokenizer

model_name = "t5-base"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

for param in model.parameters():
    param.requires_grad = False

import torch

num_classes = 3 # Replace 2 with the actual number of classes in your classification task
classifier_head = torch.nn.Linear(model.config.hidden_size, num_classes)
model.classifier = classifier_head
model.lm_head=classifier_head


from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)
criterion = torch.nn.CrossEntropyLoss()

from transformers import get_scheduler
from torch.nn import CrossEntropyLoss
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
   name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        input_ids = batch["input_ids"].to(device)
        sequence_length = input_ids.size(2)
        print(sequence_length)
        print("SHAPE: ", input_ids.shape)

        input_ids = input_ids.view(5, sequence_length)
        print(input_ids)

        attention_mask = batch["attention_mask"].to(device)
        attention_mask = attention_mask.view(5, sequence_length)

        labels = batch["labels"].to(device)
        
        print("labels: ",labels)
        ###############
        # input_ids = torch.tensor([[1, 2, 3, 4, 5]])
        # print(input_ids)
        # attention_mask = torch.tensor([[1, 1, 1, 1, 1]])
        # # Assuming batch_size = 2 and num_classes = 3
        # labels = torch.tensor([[0, 1, 2]])  # Shape: [2, 3]

        decoder_input_ids = torch.zeros_like(input_ids)

        ###############
        print(labels)
        print("Labels shape:", labels.shape)

        print(input_ids.shape, attention_mask.shape, decoder_input_ids.shape, labels.shape)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask,decoder_input_ids=decoder_input_ids)
        # print(outputs)
        # logits = outputs.logits
        # loss = criterion(logits, labels) 


        logits = outputs.logits
        print(logits)

        # Calculate loss
        loss_fn = CrossEntropyLoss()
        logits = logits.view(-1, logits.shape[-1])  # Reshape logits to [batch_size * sequence_length, num_classes]
        labels = labels.repeat(logits.shape[0] // labels.shape[0])  # Repeat labels to match logits length
        
        loss = criterion(logits, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        # Update progress bar
        progress_bar.set_postfix({"loss": loss.item()})

        # loss = outputs.loss
        # loss.backward()

        # optimizer.step()
        # lr_scheduler.step()
        # optimizer.zero_grad()
        # progress_bar.update(1)

model.eval()  # Set the model to evaluation mode

val_loss = 0.0  # Initialize the validation loss
total_predictions = []  # List to store all predictions
total_labels = []  # List to store all ground truth labels

# Iterate over the validation DataLoader
for batch in eval_dataloader:



    input_ids = batch["input_ids"].to(device)
    sequence_length = input_ids.size(2)
    print(sequence_length)
    print("SHAPE: ", input_ids.shape)

    input_ids = input_ids.view(5, sequence_length)
    print(input_ids)

    attention_mask = batch["attention_mask"].to(device)
    attention_mask = attention_mask.view(5, sequence_length)

    labels = batch["labels"].to(device)
    decoder_input_ids = torch.zeros_like(input_ids)
        

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask,decoder_input_ids=decoder_input_ids)
        logits = outputs.logits

    logits = logits.view(-1, logits.shape[-1])  # Reshape logits to [batch_size * sequence_length, num_classes]
    labels = labels.repeat(logits.shape[0] // labels.shape[0])  # Repeat labels to match logits length

    loss = criterion(logits, labels)  # Calculate the validation loss
    val_loss += loss.item()  # Accumulate the validation loss

    predicted_labels = torch.argmax(logits, dim=1)  # Get predicted labels
    total_predictions.extend(predicted_labels.tolist())  # Store predictions
    total_labels.extend(labels.tolist())  # Store ground truth labels

# Calculate the average validation loss
avg_val_loss = val_loss / len(eval_dataloader)
import numpy as np
# Convert the predictions and labels to NumPy arrays
predictions = np.array(total_predictions)
labels = np.array(total_labels)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate evaluation metrics (e.g., accuracy, precision, recall, F1 score)
accuracy = accuracy_score(labels, predictions)
precision = precision_score(labels, predictions, average="macro")
recall = recall_score(labels, predictions, average="macro")
f1 = f1_score(labels, predictions, average="macro")

# Print the evaluation results
print("Validation Loss:", avg_val_loss)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

"""MODEL DETAILS #WIP"""

logits = logits.view(-1, logits.shape[-1])  # Reshape logits to [batch_size * sequence_length, num_classes]

labels = labels.repeat(logits.shape[0] // labels.shape[0])  # Repeat labels to match logits length

# Now the shapes of logits and labels should match
print(logits.shape)  # torch.Size([7500, 3])
print(labels.shape)  # torch.Size([7500])
