# -*- coding: utf-8 -*-
"""Jun5-test-2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1snV-q2Xiaymnf4hjGeXb5deHFotiacWn
"""

# import torch
# import tensorflow as tf
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(device)
# import gc

# # Custom Callback To Include in Callbacks List At Training Time
# class GarbageCollectorCallback(tf.keras.callbacks.Callback):
#     def on_epoch_end(self, epoch, logs=None):
#         gc.collect()

# !ls snli_1.0

# #Installing the needed Libraries
# !pip install transformers
# !pip install datsets transformers[sentencepiece]
# !pip install sentencepiece
# !wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip
# !unzip snli_1.0.zip
# !cp snli_1.0/snli_1.0_train.jsonl /content/

# !cp snli_1.0/snli_1.0_dev.jsonl /content/

# !cp snli_1.0/snli_1.0_test.jsonl /content/

# !pip install --upgrade transformers

#Loading the dataset into a train variable
import json
from transformers import T5Tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-base')

with open('snli_1.0_train.jsonl', 'r') as f:
    snli_train = [json.loads(line) for line in f]

print(snli_train[:5])

#eval
with open('snli_1.0_dev.jsonl', 'r') as f:
    snli_eval = [json.loads(line) for line in f]

print(snli_eval[:5])

#test
with open('snli_1.0_test.jsonl', 'r') as f:
    snli_test = [json.loads(line) for line in f]

print(snli_test[:5])

#Temporarily taking up only 5 examples for training
snli_train=snli_train[:300]

snli_eval=snli_eval[:50]

# snli_test=snli_test[:5]

# snli_train
# snli_eval
# snli_test

print(len(snli_train))
print(len(snli_eval))

#Tokenizing completely
print("checkpoint 1")
from transformers import T5Tokenizer
import torch

tokenizer = T5Tokenizer.from_pretrained('t5-base')

#Special Tokens for concatenating the hypothesis and premise [SEP] will separate the two sentences, [CLS] is in the beginning - forgot what it does
tokenizer.add_special_tokens({'additional_special_tokens': ['[SEP]', '[CLS]']})

#Tokenizer function
def tokenize_function(example):
    label=-1
    premise = example['sentence1']
    hypothesis = example['sentence2']
    input_text = f"[CLS] premise: {premise} [SEP] hypothesis: {hypothesis}"
    encoded_inputs = tokenizer.encode_plus(
        input_text,
        padding='max_length',
        truncation=True,
        max_length=1500,
        return_tensors='pt',
        return_attention_mask=True
    )
    # print("encoded inp 1: ", encoded_inputs)

    # Map gold_label to 0, 1, and 2
    gold_label = example['gold_label']
    if gold_label == 'entailment':
        label = 0
    elif gold_label == 'contradiction':
        label = 1
    elif gold_label == 'neutral':
        label = 2

    encoded_inputs['label'] = torch.tensor(label)
    encoded_inputs['text'] = input_text

    # print("encoded inp 2:", encoded_inputs)
    return encoded_inputs

list1=[]
print("checkpoint 2")
# Tokenize the examples individually and append the values to the lists
cnt=0
for example in snli_train:
    tokenized_example = tokenize_function(example)
    list1.append(tokenized_example)
    print(cnt)
    cnt=cnt+1

# print(list1[1])


list_eval=[]
c=0
# Tokenize the examples individually and append the values to the lists
for example in snli_eval:
    tokenized_example_eval = tokenize_function(example)
    list_eval.append(tokenized_example_eval)
    print(c)
    c=c+1

# print(list_eval[1])


# list_test=[]

# # Tokenize the examples individually and append the values to the lists
# for example in snli_test:
#     tokenized_example_test = tokenize_function(example)
#     list_test.append(tokenized_example_test)

# print(list_test[1])

# Create an empty list to store the dictionaries
list_of_dicts = []
print("checkpoint 3")
# Iterate over the elements in list1 and convert each element to a dictionary
for example in list1:
    dict_example = {
        'label': example['label'],
        'text': example['text'],
        'input_ids': example['input_ids'],
        'attention_mask': example['attention_mask']
    }
    list_of_dicts.append(dict_example)


# Create an empty list to store the dictionaries
list_of_dicts_eval = []

# Iterate over the elements in list1 and convert each element to a dictionary
for example in list_eval:
    dict_example = {
        'label': example['label'],
        'text': example['text'],
        'input_ids': example['input_ids'],
        'attention_mask': example['attention_mask']
    }
    list_of_dicts_eval.append(dict_example)

# list_of_dicts
# list_of_dicts_eval

#THIS IS DATASET OF DICTS/LISTS (depends how u call it) - Correct
# !pip install datasets  
from datasets import Dataset

# Create an empty dictionary to store the data
dataset_dict = {}

# Iterate over the dictionaries in list_of_dicts and populate the dataset_dict
for i, example in enumerate(list_of_dicts):
    for key, value in example.items():
        if key not in dataset_dict:
            dataset_dict[key] = []
        dataset_dict[key].append(value)


# Convert the dataset_dict to a Hugging Face datasets object
dataset = Dataset.from_dict(dataset_dict)

# Print the dataset of dicts/lists - Correct
# print(dataset)
########EVAL
dataset_dict_eval = {}

# Iterate over the dictionaries in list_of_dicts and populate the dataset_dict
for i, example in enumerate(list_of_dicts_eval):
    for key, value in example.items():
        if key not in dataset_dict_eval:
            dataset_dict_eval[key] = []
        dataset_dict_eval[key].append(value)


# Convert the dataset_dict to a Hugging Face datasets object
dataset_eval = Dataset.from_dict(dataset_dict_eval)

# Print the dataset of dicts/lists - Correct
# print(dataset_eval)


#At this point we have a dataset of lists/dicts as we wanted, now we want datadict of dataset: i think we dont have to cz in the YELP example the datadict consist of train and val and that creates a datadict: So can wait on that

#old notes
#Needed: A datasetdict of datasets of dicts 
#Have: A datasetdict of lists of dicts

# !pip install datasets
print("checkpoint 5")
from datasets import DatasetDict

dataset_dict = DatasetDict({
    "train": dataset,
    "eval":dataset_eval

})

#Completely matches with yelp!!
print(type(dataset_dict['train']))

#We need a dataset dict of datasets
print(dataset_dict)

#Training
import torch
torch.cuda.empty_cache()
tokenized_datasets = dataset_dict.remove_columns('text')

print(tokenized_datasets)

tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

tokenized_datasets.set_format("torch")

from torch.utils.data import DataLoader

train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=10)
eval_dataloader = DataLoader(tokenized_datasets["eval"], shuffle=True, batch_size=10)
# eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)

print(train_dataloader)

# for batch in train_dataloader:
#     for key, value in batch.items():
#         print(key, value)

from transformers import T5ForConditionalGeneration, T5Tokenizer

model_name = "t5-base"
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained(model_name)

for param in model.parameters():
    param.requires_grad = False

import torch

num_classes = 3 # Replace 2 with the actual number of classes in your classification task
classifier_head = torch.nn.Linear(model.config.hidden_size, num_classes)
model.classifier = classifier_head
model.lm_head=classifier_head


from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5)
criterion = torch.nn.CrossEntropyLoss()

print(len(train_dataloader))

from transformers import get_scheduler
from torch.nn import CrossEntropyLoss
######NEW############
from torch.nn import Dropout




num_epochs = 2
batch_size=10
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
   name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))
c=0
print("model check point training 6")
model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        input_ids = batch["input_ids"].to(device)
        sequence_length = input_ids.size(2)

        input_ids = input_ids.view(batch_size, sequence_length)
        attention_mask = batch["attention_mask"].to(device)
        attention_mask = attention_mask.view(batch_size, sequence_length)

        labels = batch["labels"].to(device)
        # print(labels)

        ###############
        # input_ids = torch.tensor([[1, 2, 3, 4, 5]])
        # print(input_ids)
        # attention_mask = torch.tensor([[1, 1, 1, 1, 1]])
        # # Assuming batch_size = 2 and num_classes = 3
        # labels = torch.tensor([[0, 1, 2]])  # Shape: [2, 3]


        ###############
        
        decoder_input_ids = torch.zeros_like(input_ids)


        # print(labels)
        # print("Labels shape:", labels.shape)
        # print(input_ids.shape, attention_mask.shape, decoder_input_ids.shape, labels.shape)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask,decoder_input_ids=decoder_input_ids)
        #######NEW
        # Inside your model definition
        # Apply dropout
        # dropout = Dropout(p=0.1)
        # outputs = dropout(outputs)
        ##########
        # print(outputs)
        # logits = outputs.logits
        # loss = criterion(logits, labels) 


        logits = outputs.logits
        # print(logits)

        # Calculate loss
        # loss_fn = CrossEntropyLoss()
        logits = logits.view(-1, logits.shape[-1])  # Reshape logits to [batch_size * sequence_length, num_classes]
        labels = labels.repeat(logits.shape[0] // labels.shape[0])  # Repeat labels to match logits length
        # print(logits,labels)
        #previously used criterion:
        loss = criterion(logits, labels)
        #new change 
        # loss = loss_fn(logits, labels)


        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        # Update progress bar
        progress_bar.update(1)

        loss_value = loss.item()
        print("Loss:", loss_value)

        # print(loss)
        # progress_bar.set_postfix({"loss": loss.item()})

        # loss = outputs.loss
        # loss.backward()

        # optimizer.step()
        # lr_scheduler.step()
        # optimizer.zero_grad()
        # progress_bar.update(1)

model.eval()  # Set the model to evaluation mode
print("model checkpoint 7 eval")
val_loss = 0.0  # Initialize the validation loss
total_predictions = []  # List to store all predictions
total_labels = []  # List to store all ground truth labels

# Iterate over the validation DataLoader
for batch in eval_dataloader:



    input_ids = batch["input_ids"].to(device)
    sequence_length = input_ids.size(2)
    # print(sequence_length)
    # print("SHAPE: ", input_ids.shape)

    input_ids = input_ids.view(5, sequence_length)
    # print(input_ids)

    attention_mask = batch["attention_mask"].to(device)
    attention_mask = attention_mask.view(5, sequence_length)

    labels = batch["labels"].to(device)
    decoder_input_ids = torch.zeros_like(input_ids)
        

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask,decoder_input_ids=decoder_input_ids)
        logits = outputs.logits

    logits = logits.view(-1, logits.shape[-1])  # Reshape logits to [batch_size * sequence_length, num_classes]
    labels = labels.repeat(logits.shape[0] // labels.shape[0])  # Repeat labels to match logits length

    loss = criterion(logits, labels)  # Calculate the validation loss
    val_loss += loss.item()  # Accumulate the validation loss

    predicted_labels = torch.argmax(logits, dim=1)  # Get predicted labels
    total_predictions.extend(predicted_labels.tolist())  # Store predictions
    total_labels.extend(labels.tolist())  # Store ground truth labels

# Calculate the average validation loss
avg_val_loss = val_loss / len(eval_dataloader)
import numpy as np
# Convert the predictions and labels to NumPy arrays
predictions = np.array(total_predictions)
labels = np.array(total_labels)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate evaluation metrics (e.g., accuracy, precision, recall, F1 score)
accuracy = accuracy_score(labels, predictions)
precision = precision_score(labels, predictions, average="macro")
recall = recall_score(labels, predictions, average="macro")
f1 = f1_score(labels, predictions, average="macro")

# Print the evaluation results
print("Validation Loss:", avg_val_loss)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

"""MODEL DETAILS #WIP"""

# logits = logits.view(-1, logits.shape[-1])  # Reshape logits to [batch_size * sequence_length, num_classes]

# labels = labels.repeat(logits.shape[0] // labels.shape[0])  # Repeat labels to match logits length

# # Now the shapes of logits and labels should match
# print(logits.shape)  # torch.Size([7500, 3])
# print(labels.shape)  # torch.Size([7500])

# #WIP
# import inspect

# # Assuming your model object is named 'model'
# model_args = inspect.signature(model).parameters
# num_args = len(model_args)

# print("Number of arguments:", num_args)
# print("Argument names:", list(model_args.keys()))

# print(model)

# #Old code if needed
# for epoch in range(num_epochs):
#      print(batch)
#      for batch in train_dataloader:
#          batch = {k: v.to(device) for k, v in batch.items()}
#          outputs = model(**batch)
#          loss = outputs.loss
#          loss.backward()

#          optimizer.step()
#          lr_scheduler.step()
#          optimizer.zero_grad()
#          progress_bar.update(1)

"""Error Discussion 

"""

# #Error on Hyak
# Traceback (most recent call last):
#   File "/mmfs1/gscratch/ark/ananya27/May27.py", line 16, in <module>
#     from transformers import T5Tokenizer
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/site-packages/transformers/__init__.py", line 26, in <module>
#     from . import dependency_versions_check
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/site-packages/transformers/dependency_versions_check.py", line 17, in <module>
#     from .utils.versions import require_version, require_version_core
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/site-packages/transformers/utils/__init__.py", line 30, in <module>
#     from .generic import (
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/site-packages/transformers/utils/generic.py", line 18, in <module>
#     import inspect
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/inspect.py", line 143, in <module>
#     import linecache
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/linecache.py", line 11, in <module>
#     import tokenize
#   File "/mmfs1/gscratch/ark/ananya27/miniconda3/envs/T5_env/lib/python3.11/tokenize.py", line 35, in <module>
#     from token import *
#   File "/mmfs1/gscratch/ark/ananya27/token.py", line 8, in <module>
#     input_ids = tokenizer.batch_encode_plus(inputs, padding=True, truncation=True, return_tensors='pt')
#                 ^^^^^^^^^
# NameError: name 'tokenizer' is not defined


# #Tried uninstalling-installing the transformers library previously, started working then (one time) stopped working again using 'python script.py'
# #If i run on command line, it works but I cant do that, cz GPU
# #sbatch I cant tell whats happening and if its working. I git cloned and then sbatch run_jobs.sh declared an output txt file too. but how do i know if its running or whats happening
# #Next steps??

# #srun --- (before allenNLP) python ---open a Tmux and then do srun instead of sbatch. 
# #Do some reading up on - 'explanation regularization' (search on semantic scholar) - work that is adjusting loss functions - conform more closely to the explanation (loss function) - what are the diff things people have tried in this area. - able to experiment with this. (on a more conceptual level).
# #Open the refernece links at the end of the first page sent by Sofia